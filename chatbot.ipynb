{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❤️'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "emoji_dict = {\n",
    "    0: \":red_heart:\"}\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dict[label])\n",
    "\n",
    "label=0\n",
    "label_to_emoji(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Reading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "file_name = open('sun_chatbot.txt','r',errors='ignore')\n",
    "raw_data = file_name.read()\n",
    "raw_data=raw_data.lower()\n",
    "nltk.download('punkt')   #Using Punkt Tokenizer\n",
    "nltk.download('wordnet')  #using wordnet Dictonary\n",
    "sent_tokens= nltk.sent_tokenize(raw_data)  #Converts doc to list of sentences\n",
    "word_tokens=nltk.word_tokenize(raw_data)  #Converts doc to list of words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(Tokens):\n",
    "    return [lemmer.lemmatize(token) for token in Tokens]\n",
    "remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the greeting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEET_inputs=('hello','hi','yo','hey')\n",
    "GEET_responses=(['hi','hey','welcome to INFO SUN'])\n",
    "\n",
    "def geet(sent):\n",
    "    for word in sent.split():\n",
    "        if word.lower() in GEET_inputs:\n",
    "            return random.choice(GEET_responses)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response Genration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    bot_response=''\n",
    "    Tfidf=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
    "    tfidf=Tfidf.fit_transform(sent_tokens)\n",
    "    vals=cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat=vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf=flat[-2]\n",
    "    if (req_tfidf==0):\n",
    "        bot_response=bot_response+\"I am sorry ! I don't understand you\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response=bot_response+sent_tokens[idx]\n",
    "        return  bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARS :HI my name is TARS . Lets learn about SUN .Also, if you want to exist any time,just type Bye !\n",
      "energy from the sun\n",
      "photosynthesis\n",
      "\n",
      "\n",
      "sun\n",
      "the sun is an ordinary star, one of about 100 billion in our galaxy, the milky way.\n",
      "an au can be measured at light speed, or the time it takes for a photon of light to travel from the sun to earth.\n",
      "TARS :GoodBye ! Take care ❤️\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"TARS :HI my name is TARS . Lets learn about SUN .Also, if you want to exist any time,just type Bye !\")\n",
    "while(flag==True):\n",
    "    user_response=input()\n",
    "    user_response=user_response.lower()\n",
    "    if (user_response!='bye'):\n",
    "        sent_tokens.append(user_response)\n",
    "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "        final_word=list(set(word_tokens))\n",
    "        print(response(user_response))\n",
    "        sent_tokens.remove(user_response)\n",
    "\n",
    "\n",
    "        # if (user_response=='thanks' or user_response=='thank you'):\n",
    "        #     flag=False\n",
    "            # print('TARS : you are welcome')\n",
    "        #     break\n",
    "    #     else:\n",
    "    #         if (geet(user_response)!=None):\n",
    "    #             print('TARS :'+geet(user_response))\n",
    "    #         else:\n",
    "    #             sent_tokens.append(user_response)\n",
    "    #             word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "    #             final_word=list(set(word_tokens))\n",
    "    #             print(response(user_response))\n",
    "    #             sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"TARS :GoodBye ! Take care\",label_to_emoji(label))\n",
    "        break\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
